{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 搭建神经网络基本流程\n",
    "\n",
    "\n",
    "定义添加神经层的函数\n",
    "\n",
    "1.训练的数据\n",
    "\n",
    "\n",
    "2.定义节点准备接收数据\n",
    "\n",
    "\n",
    "3.定义神经层：隐藏层和预测层\n",
    "\n",
    "\n",
    "4.定义 loss 表达式\n",
    "\n",
    "\n",
    "5.选择 optimizer 使 loss 达到最小\n",
    "\n",
    "然后对所有变量进行初始化，通过 sess.run optimizer，迭代 1000 次进行学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# 添加层\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "   # add one more layer and return the output of this layer\n",
    "   Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "   biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "   Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "   if activation_function is None:\n",
    "       outputs = Wx_plus_b\n",
    "   else:\n",
    "       outputs = activation_function(Wx_plus_b)\n",
    "   return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.训练的数据\n",
    "# Make up some real data \n",
    "x_data = np.linspace(-1,1,300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.99331104]\n",
      " [-0.98662207]\n",
      " [-0.97993311]\n",
      " [-0.97324415]\n",
      " [-0.96655518]\n",
      " [-0.95986622]\n",
      " [-0.95317726]\n",
      " [-0.94648829]\n",
      " [-0.93979933]\n",
      " [-0.93311037]\n",
      " [-0.9264214 ]\n",
      " [-0.91973244]\n",
      " [-0.91304348]\n",
      " [-0.90635452]\n",
      " [-0.89966555]\n",
      " [-0.89297659]\n",
      " [-0.88628763]\n",
      " [-0.87959866]\n",
      " [-0.8729097 ]\n",
      " [-0.86622074]\n",
      " [-0.85953177]\n",
      " [-0.85284281]\n",
      " [-0.84615385]\n",
      " [-0.83946488]\n",
      " [-0.83277592]\n",
      " [-0.82608696]\n",
      " [-0.81939799]\n",
      " [-0.81270903]\n",
      " [-0.80602007]\n",
      " [-0.7993311 ]\n",
      " [-0.79264214]\n",
      " [-0.78595318]\n",
      " [-0.77926421]\n",
      " [-0.77257525]\n",
      " [-0.76588629]\n",
      " [-0.75919732]\n",
      " [-0.75250836]\n",
      " [-0.7458194 ]\n",
      " [-0.73913043]\n",
      " [-0.73244147]\n",
      " [-0.72575251]\n",
      " [-0.71906355]\n",
      " [-0.71237458]\n",
      " [-0.70568562]\n",
      " [-0.69899666]\n",
      " [-0.69230769]\n",
      " [-0.68561873]\n",
      " [-0.67892977]\n",
      " [-0.6722408 ]\n",
      " [-0.66555184]\n",
      " [-0.65886288]\n",
      " [-0.65217391]\n",
      " [-0.64548495]\n",
      " [-0.63879599]\n",
      " [-0.63210702]\n",
      " [-0.62541806]\n",
      " [-0.6187291 ]\n",
      " [-0.61204013]\n",
      " [-0.60535117]\n",
      " [-0.59866221]\n",
      " [-0.59197324]\n",
      " [-0.58528428]\n",
      " [-0.57859532]\n",
      " [-0.57190635]\n",
      " [-0.56521739]\n",
      " [-0.55852843]\n",
      " [-0.55183946]\n",
      " [-0.5451505 ]\n",
      " [-0.53846154]\n",
      " [-0.53177258]\n",
      " [-0.52508361]\n",
      " [-0.51839465]\n",
      " [-0.51170569]\n",
      " [-0.50501672]\n",
      " [-0.49832776]\n",
      " [-0.4916388 ]\n",
      " [-0.48494983]\n",
      " [-0.47826087]\n",
      " [-0.47157191]\n",
      " [-0.46488294]\n",
      " [-0.45819398]\n",
      " [-0.45150502]\n",
      " [-0.44481605]\n",
      " [-0.43812709]\n",
      " [-0.43143813]\n",
      " [-0.42474916]\n",
      " [-0.4180602 ]\n",
      " [-0.41137124]\n",
      " [-0.40468227]\n",
      " [-0.39799331]\n",
      " [-0.39130435]\n",
      " [-0.38461538]\n",
      " [-0.37792642]\n",
      " [-0.37123746]\n",
      " [-0.36454849]\n",
      " [-0.35785953]\n",
      " [-0.35117057]\n",
      " [-0.34448161]\n",
      " [-0.33779264]\n",
      " [-0.33110368]\n",
      " [-0.32441472]\n",
      " [-0.31772575]\n",
      " [-0.31103679]\n",
      " [-0.30434783]\n",
      " [-0.29765886]\n",
      " [-0.2909699 ]\n",
      " [-0.28428094]\n",
      " [-0.27759197]\n",
      " [-0.27090301]\n",
      " [-0.26421405]\n",
      " [-0.25752508]\n",
      " [-0.25083612]\n",
      " [-0.24414716]\n",
      " [-0.23745819]\n",
      " [-0.23076923]\n",
      " [-0.22408027]\n",
      " [-0.2173913 ]\n",
      " [-0.21070234]\n",
      " [-0.20401338]\n",
      " [-0.19732441]\n",
      " [-0.19063545]\n",
      " [-0.18394649]\n",
      " [-0.17725753]\n",
      " [-0.17056856]\n",
      " [-0.1638796 ]\n",
      " [-0.15719064]\n",
      " [-0.15050167]\n",
      " [-0.14381271]\n",
      " [-0.13712375]\n",
      " [-0.13043478]\n",
      " [-0.12374582]\n",
      " [-0.11705686]\n",
      " [-0.11036789]\n",
      " [-0.10367893]\n",
      " [-0.09698997]\n",
      " [-0.090301  ]\n",
      " [-0.08361204]\n",
      " [-0.07692308]\n",
      " [-0.07023411]\n",
      " [-0.06354515]\n",
      " [-0.05685619]\n",
      " [-0.05016722]\n",
      " [-0.04347826]\n",
      " [-0.0367893 ]\n",
      " [-0.03010033]\n",
      " [-0.02341137]\n",
      " [-0.01672241]\n",
      " [-0.01003344]\n",
      " [-0.00334448]\n",
      " [ 0.00334448]\n",
      " [ 0.01003344]\n",
      " [ 0.01672241]\n",
      " [ 0.02341137]\n",
      " [ 0.03010033]\n",
      " [ 0.0367893 ]\n",
      " [ 0.04347826]\n",
      " [ 0.05016722]\n",
      " [ 0.05685619]\n",
      " [ 0.06354515]\n",
      " [ 0.07023411]\n",
      " [ 0.07692308]\n",
      " [ 0.08361204]\n",
      " [ 0.090301  ]\n",
      " [ 0.09698997]\n",
      " [ 0.10367893]\n",
      " [ 0.11036789]\n",
      " [ 0.11705686]\n",
      " [ 0.12374582]\n",
      " [ 0.13043478]\n",
      " [ 0.13712375]\n",
      " [ 0.14381271]\n",
      " [ 0.15050167]\n",
      " [ 0.15719064]\n",
      " [ 0.1638796 ]\n",
      " [ 0.17056856]\n",
      " [ 0.17725753]\n",
      " [ 0.18394649]\n",
      " [ 0.19063545]\n",
      " [ 0.19732441]\n",
      " [ 0.20401338]\n",
      " [ 0.21070234]\n",
      " [ 0.2173913 ]\n",
      " [ 0.22408027]\n",
      " [ 0.23076923]\n",
      " [ 0.23745819]\n",
      " [ 0.24414716]\n",
      " [ 0.25083612]\n",
      " [ 0.25752508]\n",
      " [ 0.26421405]\n",
      " [ 0.27090301]\n",
      " [ 0.27759197]\n",
      " [ 0.28428094]\n",
      " [ 0.2909699 ]\n",
      " [ 0.29765886]\n",
      " [ 0.30434783]\n",
      " [ 0.31103679]\n",
      " [ 0.31772575]\n",
      " [ 0.32441472]\n",
      " [ 0.33110368]\n",
      " [ 0.33779264]\n",
      " [ 0.34448161]\n",
      " [ 0.35117057]\n",
      " [ 0.35785953]\n",
      " [ 0.36454849]\n",
      " [ 0.37123746]\n",
      " [ 0.37792642]\n",
      " [ 0.38461538]\n",
      " [ 0.39130435]\n",
      " [ 0.39799331]\n",
      " [ 0.40468227]\n",
      " [ 0.41137124]\n",
      " [ 0.4180602 ]\n",
      " [ 0.42474916]\n",
      " [ 0.43143813]\n",
      " [ 0.43812709]\n",
      " [ 0.44481605]\n",
      " [ 0.45150502]\n",
      " [ 0.45819398]\n",
      " [ 0.46488294]\n",
      " [ 0.47157191]\n",
      " [ 0.47826087]\n",
      " [ 0.48494983]\n",
      " [ 0.4916388 ]\n",
      " [ 0.49832776]\n",
      " [ 0.50501672]\n",
      " [ 0.51170569]\n",
      " [ 0.51839465]\n",
      " [ 0.52508361]\n",
      " [ 0.53177258]\n",
      " [ 0.53846154]\n",
      " [ 0.5451505 ]\n",
      " [ 0.55183946]\n",
      " [ 0.55852843]\n",
      " [ 0.56521739]\n",
      " [ 0.57190635]\n",
      " [ 0.57859532]\n",
      " [ 0.58528428]\n",
      " [ 0.59197324]\n",
      " [ 0.59866221]\n",
      " [ 0.60535117]\n",
      " [ 0.61204013]\n",
      " [ 0.6187291 ]\n",
      " [ 0.62541806]\n",
      " [ 0.63210702]\n",
      " [ 0.63879599]\n",
      " [ 0.64548495]\n",
      " [ 0.65217391]\n",
      " [ 0.65886288]\n",
      " [ 0.66555184]\n",
      " [ 0.6722408 ]\n",
      " [ 0.67892977]\n",
      " [ 0.68561873]\n",
      " [ 0.69230769]\n",
      " [ 0.69899666]\n",
      " [ 0.70568562]\n",
      " [ 0.71237458]\n",
      " [ 0.71906355]\n",
      " [ 0.72575251]\n",
      " [ 0.73244147]\n",
      " [ 0.73913043]\n",
      " [ 0.7458194 ]\n",
      " [ 0.75250836]\n",
      " [ 0.75919732]\n",
      " [ 0.76588629]\n",
      " [ 0.77257525]\n",
      " [ 0.77926421]\n",
      " [ 0.78595318]\n",
      " [ 0.79264214]\n",
      " [ 0.7993311 ]\n",
      " [ 0.80602007]\n",
      " [ 0.81270903]\n",
      " [ 0.81939799]\n",
      " [ 0.82608696]\n",
      " [ 0.83277592]\n",
      " [ 0.83946488]\n",
      " [ 0.84615385]\n",
      " [ 0.85284281]\n",
      " [ 0.85953177]\n",
      " [ 0.86622074]\n",
      " [ 0.8729097 ]\n",
      " [ 0.87959866]\n",
      " [ 0.88628763]\n",
      " [ 0.89297659]\n",
      " [ 0.89966555]\n",
      " [ 0.90635452]\n",
      " [ 0.91304348]\n",
      " [ 0.91973244]\n",
      " [ 0.9264214 ]\n",
      " [ 0.93311037]\n",
      " [ 0.93979933]\n",
      " [ 0.94648829]\n",
      " [ 0.95317726]\n",
      " [ 0.95986622]\n",
      " [ 0.96655518]\n",
      " [ 0.97324415]\n",
      " [ 0.97993311]\n",
      " [ 0.98662207]\n",
      " [ 0.99331104]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44474414]\n",
      " [ 0.54201134]\n",
      " [ 0.41180571]\n",
      " [ 0.60707073]\n",
      " [ 0.51369482]\n",
      " [ 0.46170363]\n",
      " [ 0.53364059]\n",
      " [ 0.43121836]\n",
      " [ 0.43926828]\n",
      " [ 0.44529738]\n",
      " [ 0.37411864]\n",
      " [ 0.37204817]\n",
      " [ 0.41731765]\n",
      " [ 0.23357926]\n",
      " [ 0.26525884]\n",
      " [ 0.33568357]\n",
      " [ 0.22097525]\n",
      " [ 0.36975857]\n",
      " [ 0.20515023]\n",
      " [ 0.26280381]\n",
      " [ 0.26706389]\n",
      " [ 0.23361342]\n",
      " [ 0.30355744]\n",
      " [ 0.20194707]\n",
      " [ 0.20626348]\n",
      " [ 0.14769012]\n",
      " [ 0.09987546]\n",
      " [ 0.20438582]\n",
      " [ 0.11314359]\n",
      " [ 0.07022958]\n",
      " [ 0.06028361]\n",
      " [ 0.06853185]\n",
      " [ 0.09587826]\n",
      " [ 0.11075612]\n",
      " [ 0.07399722]\n",
      " [ 0.12781689]\n",
      " [ 0.1027988 ]\n",
      " [ 0.08796531]\n",
      " [ 0.03589438]\n",
      " [ 0.03665922]\n",
      " [ 0.04860482]\n",
      " [-0.03476482]\n",
      " [ 0.03443771]\n",
      " [-0.04528132]\n",
      " [-0.01116648]\n",
      " [ 0.05698559]\n",
      " [-0.01378981]\n",
      " [-0.04946297]\n",
      " [ 0.01189597]\n",
      " [-0.09305309]\n",
      " [-0.06189167]\n",
      " [-0.07109172]\n",
      " [-0.0329727 ]\n",
      " [-0.09257049]\n",
      " [-0.15636066]\n",
      " [-0.08216281]\n",
      " [-0.09163472]\n",
      " [-0.19029036]\n",
      " [-0.13978365]\n",
      " [-0.12453126]\n",
      " [-0.10606459]\n",
      " [-0.13269241]\n",
      " [-0.1867016 ]\n",
      " [-0.25288415]\n",
      " [-0.15508244]\n",
      " [-0.19649137]\n",
      " [-0.20755462]\n",
      " [-0.17385187]\n",
      " [-0.06360635]\n",
      " [-0.259278  ]\n",
      " [-0.29014583]\n",
      " [-0.27567109]\n",
      " [-0.29203552]\n",
      " [-0.21314688]\n",
      " [-0.1786986 ]\n",
      " [-0.15476688]\n",
      " [-0.2259038 ]\n",
      " [-0.29757246]\n",
      " [-0.34723734]\n",
      " [-0.32552386]\n",
      " [-0.27179359]\n",
      " [-0.26796415]\n",
      " [-0.33818934]\n",
      " [-0.27551385]\n",
      " [-0.24758248]\n",
      " [-0.37525244]\n",
      " [-0.16802585]\n",
      " [-0.34215377]\n",
      " [-0.27944936]\n",
      " [-0.21891655]\n",
      " [-0.31444099]\n",
      " [-0.49258872]\n",
      " [-0.30471405]\n",
      " [-0.42017194]\n",
      " [-0.38037235]\n",
      " [-0.37878741]\n",
      " [-0.34738358]\n",
      " [-0.36521802]\n",
      " [-0.37634166]\n",
      " [-0.3313261 ]\n",
      " [-0.36271074]\n",
      " [-0.25250839]\n",
      " [-0.39318516]\n",
      " [-0.38375917]\n",
      " [-0.41043776]\n",
      " [-0.4057217 ]\n",
      " [-0.4186131 ]\n",
      " [-0.44032379]\n",
      " [-0.40907914]\n",
      " [-0.39640849]\n",
      " [-0.36628292]\n",
      " [-0.42993531]\n",
      " [-0.43260305]\n",
      " [-0.50105305]\n",
      " [-0.37347925]\n",
      " [-0.47314966]\n",
      " [-0.45549777]\n",
      " [-0.48454823]\n",
      " [-0.55330779]\n",
      " [-0.45431906]\n",
      " [-0.4537142 ]\n",
      " [-0.44158906]\n",
      " [-0.51087766]\n",
      " [-0.44519728]\n",
      " [-0.49494372]\n",
      " [-0.45014624]\n",
      " [-0.47880474]\n",
      " [-0.40080292]\n",
      " [-0.44786189]\n",
      " [-0.48002054]\n",
      " [-0.47253075]\n",
      " [-0.5566006 ]\n",
      " [-0.48486525]\n",
      " [-0.53178565]\n",
      " [-0.44787505]\n",
      " [-0.50589242]\n",
      " [-0.52323637]\n",
      " [-0.55387188]\n",
      " [-0.47019015]\n",
      " [-0.4895269 ]\n",
      " [-0.49987059]\n",
      " [-0.56562407]\n",
      " [-0.52382371]\n",
      " [-0.55128167]\n",
      " [-0.50423049]\n",
      " [-0.52000173]\n",
      " [-0.50182328]\n",
      " [-0.49441665]\n",
      " [-0.46168699]\n",
      " [-0.58790187]\n",
      " [-0.46923388]\n",
      " [-0.47649345]\n",
      " [-0.50268238]\n",
      " [-0.5688197 ]\n",
      " [-0.47883268]\n",
      " [-0.57707747]\n",
      " [-0.57908302]\n",
      " [-0.49614706]\n",
      " [-0.38270506]\n",
      " [-0.43933374]\n",
      " [-0.53411159]\n",
      " [-0.48747849]\n",
      " [-0.4508196 ]\n",
      " [-0.48105588]\n",
      " [-0.558555  ]\n",
      " [-0.5100013 ]\n",
      " [-0.47241882]\n",
      " [-0.48597015]\n",
      " [-0.45715422]\n",
      " [-0.54892219]\n",
      " [-0.54332721]\n",
      " [-0.46447076]\n",
      " [-0.58874481]\n",
      " [-0.48421453]\n",
      " [-0.45068442]\n",
      " [-0.48635407]\n",
      " [-0.46883712]\n",
      " [-0.49666776]\n",
      " [-0.49632698]\n",
      " [-0.45192161]\n",
      " [-0.50374565]\n",
      " [-0.40817526]\n",
      " [-0.34430479]\n",
      " [-0.51025064]\n",
      " [-0.40473279]\n",
      " [-0.47912687]\n",
      " [-0.35039213]\n",
      " [-0.44262073]\n",
      " [-0.36988147]\n",
      " [-0.45158596]\n",
      " [-0.40989964]\n",
      " [-0.50462097]\n",
      " [-0.43412085]\n",
      " [-0.46630705]\n",
      " [-0.46023723]\n",
      " [-0.38436531]\n",
      " [-0.2983127 ]\n",
      " [-0.40475047]\n",
      " [-0.42784575]\n",
      " [-0.34776838]\n",
      " [-0.4412408 ]\n",
      " [-0.28612719]\n",
      " [-0.30012502]\n",
      " [-0.38722013]\n",
      " [-0.36337324]\n",
      " [-0.3515699 ]\n",
      " [-0.30365529]\n",
      " [-0.36520537]\n",
      " [-0.22475026]\n",
      " [-0.37855364]\n",
      " [-0.31394111]\n",
      " [-0.32282737]\n",
      " [-0.4222046 ]\n",
      " [-0.32469296]\n",
      " [-0.32575735]\n",
      " [-0.32968123]\n",
      " [-0.30561705]\n",
      " [-0.19594477]\n",
      " [-0.29894764]\n",
      " [-0.36743559]\n",
      " [-0.24956995]\n",
      " [-0.25643154]\n",
      " [-0.14160008]\n",
      " [-0.30299091]\n",
      " [-0.24601822]\n",
      " [-0.1836439 ]\n",
      " [-0.19136583]\n",
      " [-0.22784283]\n",
      " [-0.26608934]\n",
      " [-0.16937262]\n",
      " [-0.19774748]\n",
      " [-0.24502814]\n",
      " [-0.29171361]\n",
      " [-0.14180777]\n",
      " [-0.23710785]\n",
      " [-0.22707895]\n",
      " [-0.15444046]\n",
      " [-0.1457237 ]\n",
      " [-0.14352712]\n",
      " [-0.12099271]\n",
      " [-0.05952191]\n",
      " [-0.14639993]\n",
      " [-0.11423748]\n",
      " [-0.06018266]\n",
      " [-0.14960459]\n",
      " [-0.05552484]\n",
      " [ 0.01907654]\n",
      " [-0.0800898 ]\n",
      " [-0.13852281]\n",
      " [-0.02098895]\n",
      " [-0.04035963]\n",
      " [-0.06860511]\n",
      " [ 0.05230471]\n",
      " [ 0.02428519]\n",
      " [-0.03057657]\n",
      " [-0.01521422]\n",
      " [-0.00827312]\n",
      " [-0.03227496]\n",
      " [-0.01463859]\n",
      " [ 0.06578825]\n",
      " [ 0.15852376]\n",
      " [ 0.111071  ]\n",
      " [-0.02040581]\n",
      " [ 0.0222856 ]\n",
      " [ 0.057629  ]\n",
      " [ 0.08396287]\n",
      " [ 0.12559272]\n",
      " [ 0.10227001]\n",
      " [ 0.07164227]\n",
      " [ 0.10140375]\n",
      " [ 0.22204374]\n",
      " [ 0.18669053]\n",
      " [ 0.23975768]\n",
      " [ 0.190944  ]\n",
      " [ 0.18168255]\n",
      " [ 0.2238296 ]\n",
      " [ 0.31617518]\n",
      " [ 0.21312613]\n",
      " [ 0.24808733]\n",
      " [ 0.14206479]\n",
      " [ 0.26699244]\n",
      " [ 0.27232274]\n",
      " [ 0.33980947]\n",
      " [ 0.27069957]\n",
      " [ 0.31648793]\n",
      " [ 0.38710563]\n",
      " [ 0.27708373]\n",
      " [ 0.39162354]\n",
      " [ 0.38944013]\n",
      " [ 0.25072283]\n",
      " [ 0.32587077]\n",
      " [ 0.41566279]\n",
      " [ 0.40513559]\n",
      " [ 0.36771661]\n",
      " [ 0.36484165]\n",
      " [ 0.42929571]\n",
      " [ 0.55085   ]\n",
      " [ 0.45831816]\n",
      " [ 0.57351268]\n",
      " [ 0.51746322]]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.定义节点准备接收数据\n",
    "# define placeholder for inputs to network  \n",
    "xs = tf.placeholder(tf.float32, [None, 1])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.定义神经层：隐藏层和预测层\n",
    "# add hidden layer 输入值是 xs，在隐藏层有 10 个神经元   \n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果\n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.定义 loss 表达式:以均方误差作为损失函数\n",
    "# the error between prediciton and real data    \n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                    reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.选择 optimizer 使 loss 达到最小:通过梯度下降的方法求解最佳参数                   \n",
    "# 这一行定义了用什么方式去减少 loss，学习率是 0.1       \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important step 对所有变量进行初始化\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "# 上面定义的都没有运算，直到 sess.run 才会开始运算\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.252824\n",
      "0.0185686\n",
      "0.00775317\n",
      "0.00571724\n",
      "0.00486989\n",
      "0.00443038\n",
      "0.00418099\n",
      "0.0040164\n",
      "0.00390997\n",
      "0.0038332\n",
      "0.00377065\n",
      "0.00371045\n",
      "0.0036581\n",
      "0.00360994\n",
      "0.00356178\n",
      "0.00351628\n",
      "0.00348065\n",
      "0.0034512\n",
      "0.00342797\n",
      "0.00340229\n"
     ]
    }
   ],
   "source": [
    "# 迭代 1000 次学习，sess.run optimizer\n",
    "for i in range(1000):\n",
    "   # training train_step 和 loss 都是由 placeholder 定义的运算，所以这里要用 feed 传入参数\n",
    "   sess.run(train_step, feed_dict={xs: x_data, ys: y_data})\n",
    "   if i % 50 == 0:\n",
    "       # to see the step improvement\n",
    "       print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 主要步骤的解释\n",
    "\n",
    "\n",
    "2.1 导入模块\n",
    "\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "2.2 导入或者随机定义训练的数据 x 和 y\n",
    "\n",
    "```python \n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data*0.1 + 0.3\n",
    "\n",
    "```\n",
    "\n",
    "2.3 先定义出参数 Weights，biases，拟合公式 y，误差公式 loss\n",
    "\n",
    "```python\n",
    "Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "biases = tf.Variable(tf.zeros([1]))\n",
    "y = Weights*x_data + biases\n",
    "loss = tf.reduce_mean(tf.square(y-y_data))\n",
    "```\n",
    "\n",
    "2.4 选择 Gradient Descent 这个最基本的 Optimizer\n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "```\n",
    "\n",
    "2.5 神经网络的 key idea，就是让 loss 达到最小\n",
    "\n",
    "```python\n",
    "train = optimizer.minimize(loss)\n",
    "```\n",
    "2.6 前面是定义，在运行模型前先要初始化所有变量\n",
    "\n",
    "```python \n",
    "init = tf.initialize_all_variables()\n",
    "```\n",
    "\n",
    "2.7 接下来把结构激活，sesseion像一个指针指向要处理的地方\n",
    "\n",
    "```python\n",
    "sess = tf.Session()\n",
    "```\n",
    "\n",
    "2.8 init 就被激活了，不要忘记激活\n",
    "\n",
    "```python\n",
    "init 就被激活了，不要忘记激活\n",
    "```\n",
    "\n",
    "2.9 训练201步\n",
    "\n",
    "```python\n",
    "for step in range(201):\n",
    "```\n",
    "2.10 要训练 train，也就是 optimizer：\n",
    "```python\n",
    "sess.run(train)\n",
    "\n",
    "```\n",
    "\n",
    "2.10 每 20 步打印一下结果，sess.run 指向 Weights，biases 并被输出\n",
    "\n",
    "```python\n",
    "if step % 20 == 0:\n",
    "print(step, sess.run(Weights), sess.run(biases))\n",
    "```\n",
    "所以关键的就是 y，loss，optimizer 是如何定义的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TensorFlow 基本概念及代码\n",
    "\n",
    "## Session\n",
    "\n",
    "### 矩阵乘法：tf.matmul\n",
    "\n",
    "```python\n",
    "product = tf.matmul(matrix1, matrix2) # matrix multiply np.dot(m1, m2)\n",
    "```\n",
    "\n",
    "定义 Session，它是个对象，注意大写：\n",
    "\n",
    "\n",
    "```python\n",
    "sess = tf.Session()\n",
    "```\n",
    "\n",
    "result 要去 sess.run 那里取结果：\n",
    "\n",
    "\n",
    "```python\n",
    "result = sess.run(product)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Variable\n",
    "\n",
    "用 tf.Variable 定义变量，与python不同的是，必须先定义它是一个变量，它才是一个变量，初始值为0，还可以给它一个名字 counter：\n",
    "\n",
    "\n",
    "```python\n",
    "state = tf.Variable(0, name='counter')\n",
    "```\n",
    "\n",
    "将 new_value 加载到 state 上，counter就被更新：\n",
    "\n",
    "\n",
    "```python\n",
    "update = tf.assign(state, new_value)\n",
    "```\n",
    "\n",
    "如果有变量就一定要做初始化：\n",
    "\n",
    "\n",
    "```python\n",
    "init = tf.initialize_all_variables() # must have if define variable\n",
    "```\n",
    "\n",
    "\n",
    "## placeholder：\n",
    "\n",
    "要给节点输入数据时用 placeholder，在 TensorFlow 中用placeholder 来描述等待输入的节点，只需要指定类型即可，然后在执行节点的时候用一个字典\n",
    "\n",
    "来“喂”这些节点。相当于先把变量 hold 住，然后每次从外部传入data，注意 placeholder 和 feed_dict 是绑定用的。\n",
    "\n",
    "\n",
    "这里简单提一下 feed 机制， 给 feed 提供数据，作为 run()\n",
    "\n",
    "\n",
    "调用的参数， feed 只在调用它的方法内有效, 方法结束, feed 就会消失。\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "ouput = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    " print(sess.run(ouput, feed_dict={input1: [7.], input2: [2.]}))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 神经网络基本概念\n",
    "\n",
    "激励函数：\n",
    "\n",
    "\n",
    "例如一个神经元对猫的眼睛敏感，那当它看到猫的眼睛的时候，就被激励了，相应的参数就会被调优，它的贡献就会越大。\n",
    "\n",
    "\n",
    "下面是几种常见的激活函数：\n",
    "\n",
    "x轴表示传递过来的值，y轴表示它传递出去的值：RELU ,Sigmoid 等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加神经层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def add_layer(inputs, in_size, out_size,  activation_function=None):\n",
    "\n",
    "  Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "  biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
    "  Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "\n",
    "  if activation_function is None:\n",
    "    outputs = Wx_plus_b\n",
    "  else:\n",
    "    outputs = activation_function(Wx_plus_b)\n",
    "    return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类问题的 loss 函数 cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the error between prediction and real data\n",
    "# loss 函数用 cross entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\n",
    "                                              reduction_indices=[1]))       # loss\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting\n",
    "\n",
    "Tensorflow 有一个很好的工具, 叫做dropout, 只需要给予它一个不被 drop 掉的百分比，就能很好地降低 overfitting。\n",
    "\n",
    "\n",
    "dropout 是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络。\n",
    "\n",
    "代码实现就是在 add layer 函数里加上 dropout, keep_prob 就是保持多少不被 drop，在迭代时在 sess.run 中被 feed。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(inputs, in_size, out_size, layer_name, activation_function=None, ):\n",
    "    # add one more layer and return the output of this layer\n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size]))\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, )\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    \n",
    "    # here to dropout\n",
    "    # 在 Wx_plus_b 上drop掉一定比例\n",
    "    # keep_prob 保持多少不被drop，在迭代时在 sess.run 中 feed\n",
    "    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)\n",
    "    \n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b, )\n",
    "    tf.histogram_summary(layer_name + '/outputs', outputs)  \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 可视化 Tensorboard\n",
    "\n",
    "\n",
    "Tensorflow 自带 tensorboard ，可以自动显示我们所建造的神经网络流程图\n",
    "\n",
    "就是用 with tf.name_scope 定义各个框架，注意看代码注释中的区别：\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    # add one more layer and return the output of this layer\n",
    "    # 区别：大框架，定义层 layer，里面有 小部件\n",
    "    with tf.name_scope('layer'):\n",
    "        # 区别：小部件\n",
    "        with tf.name_scope('weights'):\n",
    "            Weights = tf.Variable(tf.random_normal([in_size, out_size]), name='W')\n",
    "        with tf.name_scope('biases'):\n",
    "            biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name='b')\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            Wx_plus_b = tf.add(tf.matmul(inputs, Weights), biases)\n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b, )\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# define placeholder for inputs to network\n",
    "# 区别：大框架，里面有 inputs x，y\n",
    "with tf.name_scope('inputs'):\n",
    "    xs = tf.placeholder(tf.float32, [None, 1], name='x_input')\n",
    "    ys = tf.placeholder(tf.float32, [None, 1], name='y_input')\n",
    "\n",
    "# add hidden layer\n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer\n",
    "prediction = add_layer(l1, 10, 1, activation_function=None)\n",
    "\n",
    "# the error between prediciton and real data\n",
    "# 区别：定义框架 loss\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),\n",
    "                                        reduction_indices=[1]))\n",
    "\n",
    "# 区别：定义框架 train\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# 区别：sess.graph 把所有框架加载到一个文件中放到文件夹\"logs/\"里 \n",
    "# 接着打开terminal，进入你存放的文件夹地址上一层，运行命令 tensorboard --logdir='logs/'\n",
    "# 会返回一个地址，然后用浏览器打开这个地址，在 graph 标签栏下打开\n",
    "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "# important step\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 保存和加载\n",
    "\n",
    "\n",
    "训练好了一个神经网络后，可以保存起来下次使用时再次加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to path:  my_model/save_net.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "## Save to file\n",
    "# remember to define the same dtype and shape when restore\n",
    "W = tf.Variable([[1,2,3],[3,4,5]], dtype=tf.float32, name='weights')\n",
    "b = tf.Variable([[1,2,3]], dtype=tf.float32, name='biases')\n",
    "\n",
    "init= tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# 用 saver 将所有的 variable 保存到定义的路径\n",
    "with tf.Session() as sess:\n",
    "   sess.run(init)\n",
    "   save_path = saver.save(sess, \"my_model/save_net.ckpt\")\n",
    "   print(\"Save to path: \", save_path)\n",
    "\n",
    "\n",
    "################################################\n",
    "\n",
    "# restore variables\n",
    "# redefine the same shape and same type for your variables\n",
    "W = tf.Variable(np.arange(6).reshape((2, 3)), dtype=tf.float32, name=\"weights\")\n",
    "b = tf.Variable(np.arange(3).reshape((1, 3)), dtype=tf.float32, name=\"biases\")\n",
    "\n",
    "# not need init step\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "# # 用 saver 从路径中将 save_net.ckpt 保存的 W 和 b restore 进来\n",
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"my_model/save_net.ckpt\")\n",
    "#     print(\"weights:\", sess.run(W))\n",
    "#     print(\"biases:\", sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 现在只能保存 variables，还不能保存整个神经网络的框架，所以再使用的时候，需要重新定义框架，然后把 variables 放进去学习。\n",
    "参考链接：https://www.jianshu.com/p/e112012a4b2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
